{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные\n",
    "\n",
    "Данные в [архиве](https://drive.google.com/file/d/15o7fdxTgndoy6K-e7g8g1M2-bOOwqZPl/view?usp=sharing). В нём два файла:\n",
    "- `news_train.txt` тестовое множество\n",
    "- `news_test.txt` тренировочное множество\n",
    "\n",
    "С некоторых новостных сайтов были загружены тексты новостей за период  несколько лет, причем каждая новость принаделжит к какой-то рубрике: `science`, `style`, `culture`, `life`, `economics`, `business`, `travel`, `forces`, `media`, `sport`.\n",
    "\n",
    "В каждой строке файла содержится метка рубрики, заголовок новостной статьи и сам текст статьи, например:\n",
    "\n",
    ">    **sport**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею разгромила чехов**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею крупно об...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача\n",
    "\n",
    "1. Обработать данные, получив для каждого текста набор токенов\n",
    "Обработать токены с помощью (один вариант из трех):\n",
    "    - pymorphy2\n",
    "    - русского [snowball стеммера](https://www.nltk.org/howto/stem.html)\n",
    "    - [SentencePiece](https://github.com/google/sentencepiece) или [Huggingface Tokenizers](https://github.com/huggingface/tokenizers)\n",
    "    \n",
    "    \n",
    "2. Обучить word embeddings (fastText, word2vec, gloVe) на тренировочных данных. Можно использовать [gensim](https://radimrehurek.com/gensim/models/word2vec.html) . Продемонстрировать семантические ассоциации. \n",
    "\n",
    "3. Реализовать алгоритм классификации документа по категориям, посчитать точноть на тестовых данных, подобрать гиперпараметры. Метод векторизации выбрать произвольно - можно использовать $tf-idf$ с понижением размерности (см. scikit-learn), можно использовать обученные на предыдущем шаге векторные представления, можно использовать [предобученные модели](https://rusvectores.org/ru/models/). Имейте ввиду, что простое \"усреднение\" токенов в тексте скорее всего не даст положительных результатов. Нужно реализовать два алгоритмов из трех:\n",
    "     - SVM\n",
    "     - наивный байесовский классификатор\n",
    "     - логистическая регрессия\n",
    "    \n",
    "\n",
    "4.* Реализуйте классификацию с помощью нейросетевых моделей. Например [RuBERT](http://docs.deeppavlov.ai/en/master/features/models/bert.html) или [ELMo](https://rusvectores.org/ru/models/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import warnings\n",
    "import random\n",
    "import re\n",
    "import pymorphy2\n",
    "import pandas as pd\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "from collections import Counter\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "             tag                                             header  \\\n0          sport  Овечкин пожертвовал детской хоккейной школе ав...   \n1        culture    Рекордно дорогую статую майя признали подделкой   \n2        science   Samsung представила флагман в защищенном корпусе   \n3          sport  С футболиста «Спартака» сняли четырехматчевую ...   \n4          media            Hopes & Fears объединится с The Village   \n...          ...                                                ...   \n14995       life  Составлен рейтинг лучших европейских пляжей 20...   \n14996      media          В «Снобе» объяснили причину смены формата   \n14997  economics  Минфин предложил штрафовать за биткоины на 50 ...   \n14998       life  Мэл Гибсон заплатит бывшей подруге 750 тысяч д...   \n14999      media  Еще на двух линиях московского метро заработал...   \n\n                                                 content  \n0      Нападающий «Вашингтон Кэпиталз» Александр Овеч...  \n1      Власти Мексики объявили подделкой статую майя,...  \n2      Южнокорейская Samsung анонсировала защищенную ...  \n3      Контрольно-дисциплинарный комитет (КДК) РФС сн...  \n4      Интернет-издание Hopes & Fears объявило о свое...  \n...                                                  ...  \n14995  Опубликован рейтинг лучших европейских пляжей ...  \n14996  Генеральный директор «Сноб медиа» Марина Гевор...  \n14997  Минфин разработал законопроект, устанавливающи...  \n14998  Актер и режиссер Мэл Гибсон выплатит своей быв...  \n14999  На Серпуховско-Тимирязевской и Бутовской линия...  \n\n[15000 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tag</th>\n      <th>header</th>\n      <th>content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sport</td>\n      <td>Овечкин пожертвовал детской хоккейной школе ав...</td>\n      <td>Нападающий «Вашингтон Кэпиталз» Александр Овеч...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>culture</td>\n      <td>Рекордно дорогую статую майя признали подделкой</td>\n      <td>Власти Мексики объявили подделкой статую майя,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>science</td>\n      <td>Samsung представила флагман в защищенном корпусе</td>\n      <td>Южнокорейская Samsung анонсировала защищенную ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sport</td>\n      <td>С футболиста «Спартака» сняли четырехматчевую ...</td>\n      <td>Контрольно-дисциплинарный комитет (КДК) РФС сн...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>media</td>\n      <td>Hopes &amp; Fears объединится с The Village</td>\n      <td>Интернет-издание Hopes &amp; Fears объявило о свое...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>14995</th>\n      <td>life</td>\n      <td>Составлен рейтинг лучших европейских пляжей 20...</td>\n      <td>Опубликован рейтинг лучших европейских пляжей ...</td>\n    </tr>\n    <tr>\n      <th>14996</th>\n      <td>media</td>\n      <td>В «Снобе» объяснили причину смены формата</td>\n      <td>Генеральный директор «Сноб медиа» Марина Гевор...</td>\n    </tr>\n    <tr>\n      <th>14997</th>\n      <td>economics</td>\n      <td>Минфин предложил штрафовать за биткоины на 50 ...</td>\n      <td>Минфин разработал законопроект, устанавливающи...</td>\n    </tr>\n    <tr>\n      <th>14998</th>\n      <td>life</td>\n      <td>Мэл Гибсон заплатит бывшей подруге 750 тысяч д...</td>\n      <td>Актер и режиссер Мэл Гибсон выплатит своей быв...</td>\n    </tr>\n    <tr>\n      <th>14999</th>\n      <td>media</td>\n      <td>Еще на двух линиях московского метро заработал...</td>\n      <td>На Серпуховско-Тимирязевской и Бутовской линия...</td>\n    </tr>\n  </tbody>\n</table>\n<p>15000 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\n",
    "    \"news_train.txt\",\n",
    "    encoding=\"utf8\",\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=['tag', 'header', 'content']\n",
    ")\n",
    "\n",
    "test = pd.read_csv(\n",
    "    \"news_test.txt\",\n",
    "    encoding=\"utf8\",\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=['tag', 'header', 'content']\n",
    ")\n",
    "\n",
    "train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## №1 Обработка"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def process_sentences(df):\n",
    "    processed_texts = []\n",
    "    for i, text in enumerate(df['content']):\n",
    "        text = text.lower()\n",
    "        words = re.findall(r'\\b\\w+\\b', text)\n",
    "        tokens = [morph.parse(word)[0].normal_form for word in words]\n",
    "        processed_texts.append(tokens)\n",
    "    return processed_texts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess train...\n",
      "Preprocess test...\n"
     ]
    }
   ],
   "source": [
    "print('Preprocess train...')\n",
    "processed_train = process_sentences(train)\n",
    "print('Preprocess test...')\n",
    "processed_test = process_sentences(test)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## №2 Word2vec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=processed_train, window=5, min_count=1, workers=7)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "(12882568, 14796365)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(processed_train, total_examples=10, epochs=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "wv = model.wv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "[('инициатива', 0.8210356831550598),\n ('политический', 0.7912275791168213),\n ('экономический', 0.7823328375816345),\n ('обеспокоенность', 0.7801803946495056),\n ('коммуникация', 0.7747455835342407)]"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(positive=['политика'], topn=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "[('apple', 0.7341917157173157)]"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(positive=['samsung', \"сша\"], negative=['корея'], topn=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## №3 Классификация"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "def print_metrics(y_pred, y_test):\n",
    "    print(f'Precision: {precision_score(y_pred, y_test, average=\"weighted\"):.2f}')\n",
    "    print(f'Recall: {recall_score(y_pred, y_test, average=\"weighted\"):.2f}')\n",
    "    print(f'F1 score: {f1_score(y_pred, y_test, average=\"weighted\"):.2f}')\n",
    "    print(f'Accuracy: {accuracy_score(y_pred, y_test):.2f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "X_train = [' '.join(sentence) for sentence in processed_train]\n",
    "y_train = train['tag']\n",
    "X_test = [' '.join(sentence) for sentence in processed_test]\n",
    "y_test = test['tag']\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X_train_vec = tfidf.fit_transform(X_train)\n",
    "X_test_vec = tfidf.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "log_reg = RandomizedSearchCV(\n",
    "    LogisticRegression(),\n",
    "    param_distributions={\n",
    "        'C': np.arange(0.01, 1.01, 0.01)\n",
    "    }\n",
    ")\n",
    "\n",
    "naive_bayes = RandomizedSearchCV(\n",
    "    MultinomialNB(),\n",
    "    param_distributions={\n",
    "        'alpha': np.arange(0.5, 1.51, 0.01)\n",
    "    }\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "RandomizedSearchCV(estimator=LogisticRegression(),\n                   param_distributions={'C': array([0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 , 0.11,\n       0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21, 0.22,\n       0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32, 0.33,\n       0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43, 0.44,\n       0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54, 0.55,\n       0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65, 0.66,\n       0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77,\n       0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88,\n       0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99,\n       1.  ])})"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.fit(X_train_vec, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "RandomizedSearchCV(estimator=MultinomialNB(),\n                   param_distributions={'alpha': array([0.5 , 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6 ,\n       0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7 , 0.71,\n       0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8 , 0.81, 0.82,\n       0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9 , 0.91, 0.92, 0.93,\n       0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1.  , 1.01, 1.02, 1.03, 1.04,\n       1.05, 1.06, 1.07, 1.08, 1.09, 1.1 , 1.11, 1.12, 1.13, 1.14, 1.15,\n       1.16, 1.17, 1.18, 1.19, 1.2 , 1.21, 1.22, 1.23, 1.24, 1.25, 1.26,\n       1.27, 1.28, 1.29, 1.3 , 1.31, 1.32, 1.33, 1.34, 1.35, 1.36, 1.37,\n       1.38, 1.39, 1.4 , 1.41, 1.42, 1.43, 1.44, 1.45, 1.46, 1.47, 1.48,\n       1.49, 1.5 ])})"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes.fit(X_train_vec, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "Precision: 0.89\n",
      "Recall: 0.88\n",
      "F1 score: 0.88\n",
      "Accuracy: 0.88\n",
      "\n",
      "Naive Bayes:\n",
      "Precision: 0.88\n",
      "Recall: 0.82\n",
      "F1 score: 0.85\n",
      "Accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "print('Logistic Regression:')\n",
    "y_pred_lreg = log_reg.best_estimator_.predict(X_test_vec)\n",
    "print_metrics(y_pred_lreg, y_test)\n",
    "\n",
    "print()\n",
    "print('Naive Bayes:')\n",
    "y_pred_nb = naive_bayes.predict(X_test_vec)\n",
    "print_metrics(y_pred_nb, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Вывод: логистическая регрессия лучше"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}